###**“横看成岭侧成峰，远近高低各不同。”**

支持向量机（Support Vector Machine, SVM）作为一个被广泛应用的有监督机器学习算法，网络上对它的介绍数不胜数，其中更有不少好文佳作。本文与它们的区别在于：并不着重于“教程式”地对SVM进行系统性介绍，而是希望从三个不同的角度对这个算法进行探究。我相信经过这番“把玩”，看过你会跟我一样觉得：机器学习真的是好玩！

----------

 

1、引言
----

最大化类间间隔分类器（maximum margin classifier），估计是最为直观，也是最为人们所熟悉的对于SVM的理解。我们不妨也先从这个角度切入，看看为什么SVM能给我们带来优良的泛化能力。这一部分的路线图如下：
![路线图](http://img.blog.csdn.net/20150703144058086)

2、线性可分和线性分类器
------

对于一个二分类问题，如果存在至少一个超平面能够将不同类别的样本分开，我们就说这些样本是线性可分的（linear separable）。所谓超平面，就是一个比原特征空间少一个维度的子空间，在二维情况下就是一条直线，在三维情况下就是一个平面。

线性分类器（linear classifier）是一类通过将样本特征进行线性组合来作出分类决策的算法，它的目标就是找到一个如上所述能够分割不同类别样本的超平面。这样在预测的时候，我们就可以根据样本位于超平面的哪一边来作出决策。

用数学语言来描述，一个线性函数可以简单表示为：$f(x) = w^Tx+b$，而线性分类器则根据线性函数的结果进行分类决策：$$y=g(f(x))=g(w^Tx+b)$$ 其中$g(\cdot)$是一个将变量映射到不同类别的非线性函数，可以简单取为：
$$
g(z) =
\begin{cases}
1,  & \text{if $z\geq0$} \\[2ex]
-1, & \text{if $z<0$}
\end{cases}
$$
即分类的结果由 $f(x)$ 的符号决定，$f(x)=w^Tx+b＝0$即为分类超平面。

下图展示了几个线性可分／不可分的例子，并且画出了一个可能的分类超平面：
![图片来源](http://img.blog.csdn.net/20150703224118667)

3、最大化间隔
-------

在样本线性可分的情况下，可行的分类超平面可能会有很多，如下图的$L_1$、$L_2$和$L_3$。
![这里写图片描述](http://img.blog.csdn.net/20150704121909230)

那么怎么选择一个最好的呢？从上图我们可以直观看出，$L_2$比另外两条分界线要更好，这是因为$L_2$离样本的距离更远一些，让人觉得确信度更高。这好比人（相当于样本）站在离悬崖边（分类边界）越远，人就会感到越安全（分类结果是安全还是危险）。从统计的角度讲，由于正负样本可以看作从两个不同的分布随机抽样而得，若分类边界与两个分布的距离越大，抽样出的样本落在分类边界另一边的概率就会越小。

SVM正是基于这种直观思路来确定最佳分类超平面的：通过选取能够最大化类间间隔的超平面，得到一个具有高确信度和泛化能力的分类器，即最大间隔分类器。

##3.1、间隔
既然SVM的目标是最大化间隔，我们便要先对“间隔”进行定义。所谓间隔，就是分类超平面与所有样本距离的最小值，表示为：$$\gamma = min\{dist(l, x_i)\  | \ i=1,2,...,N\}$$ 其中$l$表示分类超平面，$N$为样本个数，$x_i$为第$i$个样本。接下来我们还需要定义样本到超平面的“距离” $dist(l, x)$。

假设任意一个样本点$x_0$，其在分类超平面上的投影记作$\hat{x}_0$。对于分类超平面$w^Tx+b=0$，我们知道他的法向量是$w$，法向量的方向可以由法向量除以其模长所得：$\frac{w}{\|w\|}$。我们将$dist(l, x_i)$记为$d$（$d\geq0$），则可以得到：$$x_0-\hat{x}_0=d\frac{w}{\|w\|}$$等式两边同时左乘$w^T$并加上b，并且利用超平面上的点$w^T\hat{x}_0=0$的性质，我们可以得到：$$d=\frac{\left|w^Tx+b\right|}{\|w\|}$$记$y\in\{-1,1\}$为分类标签，由于$y(w^Tx+b)=\left|w^Tx+b\right|$，我们可以以此消去上式的绝对值。

综上所述，我们可以得到对于分类超平面$l$和$N$个样本$x_i$的“间隔”的表达式：$$\gamma=min\left\{\frac{y_i(w^Tx_i+b)}{\|w\|}\big|\ i=1,2,...,N\right\}$$

##3.2、最大化

有了上述定义的间隔，接下来的事情就很直接了——求解能使间隔最大化的参数$w$和$b$，即求解以下优化函数：$$\max_{w,b}\ \gamma=min\left\{\frac{y_i(w^Tx_i+b)}{\|w\|}\big|\ i=1,2,...,N\right\}$$令$\frac{y_0(w^Tx_0+b)}{\|w\|}=\gamma$，上述优化函数也可以写成如下等价的形式：$$\max_{w,b}\ \frac{y_0(w^Tx_0+b)}{\|w\|}$$ $$s.t.\ y_i(w^Tx_i+b)\geq y_0(w^Tx_0+b), \  i=1,2,...,N$$其中第二行的约束条件是为了满足对“间隔”的定义。下面我们来做一些数学上的小变换，使形式更为简洁。

我们定义$\hat{w}={w}\ /\ {y_0(w^Tx_0+b)}$，$\hat{b}={b}\ /\ {y_0(w^Tx_0+b)}$，则目标函数可写成：${1}\ /\ {\|\hat{w}\|}$，约束条件可写成：$y_i(w^Tx_i+b)\geq1, \  i=1,2,...,N$。再用$w$替换$\hat{w}$，并且利用$\max_{w}{1}\ /\ {\|w\|}$与$\max_{w}{1}\ /\ {\|w\|^2}$等价的原理，可以得到以下下等价的优化问题：$$\max_{w,b}\ \frac{1}{\|w\|^2}$$ $$s.t. \ y_i(w^Tx_i+b)\geq 1, \  i=1,2,...,N$$

4、松弛变量
------

以上我们都只关心一个目的：寻找能够最大化间隔的分类超平面。然而，由于样本点中异常点的存在，只考虑这一个因素往往无法得到一个最佳的分类器。我们来看下图的例子：
![松弛变量](http://img.blog.csdn.net/20150704110932012)

从上图可以看出：若我们严格遵守“间隔”的限制，由于蓝色异常点的存在，最终只能得到一个间隔很小的分类超平面。反之，如果我们能够放宽对于间隔的限制，便可以一定程度的忽略异常点的影响，反而能得到间隔更大的分类超平面。

上述容忍异常点的思路可以通过引入“松弛变量”（slack variable）实现。在原优化问题中，我们对“间隔”的限制表现在 $y_i(w^Tx_i+b)\geq1, \  i=1,2,...,N$ 当中。为了放宽对此的限制，我们对每个样本引入其对应的松弛变量 $\zeta_i\ (\zeta_i\geq0)$，则限制条件变为：$$y_i(w^Tx_i+b)\geq1-\zeta_i, \  i=1,2,...,N$$从上式可以看出，若样本点$x_i$不是异常点（满足 $y_i(w^Tx_i+b)\geq1$），则其松弛变量$\zeta_i=0$，与原限制一样。若样本点$x_i$是异常点，只要$\zeta_i$足够大，限制条件便能满足，分类超平面（由$w$、$b$决定）不受影响。直观上讲，$\zeta_i$等于将异常点“拉”回原间隔处所需要移动的距离，如下图所示：
![松弛变量2](http://img.blog.csdn.net/20150704113736003)

松弛变量的引入有助于增强模型对异常点的容忍能力，还能解决一定的数据线性不可分的问题。然而，如果不对松弛变量进行限制，得到的分类器又会变得没有用处（大量的错误分类）。因此，我们需要同时对两个目标进行优化：最大化间隔和容忍异常样本，并且引入一个平衡参数 $C$ （$C\geq0$）来衡量这两个方面的重要程度。引入松弛变量后的完整优化问题如下：$$\max_{w,b}\ \frac{1}{\|w\|^2}-C\sum_i\zeta_i$$ $$s.t. \ y_i(w^Tx_i+b)\geq 1-\zeta_i, \  i=1,2,...,N$$ $$\zeta_i\geq0$$

最后，我们再来分析一下平衡参数 $C$ 对求得分类超平面的影响。

 - $C$ 取无穷：$\zeta_i$只能为零，代表无法容忍任何误判样本的出现，即严格遵守“间隔”的限制，得到没有引入松弛变量时的分类超平面
 - $C$取零：$\zeta_i$可以任意大，即任何误判结果都可以被容忍，得到分类超平面没有意义
 - $C$较大：$\zeta_i$不能很大，因此限制条件难以被忽略，会得到较为狭窄间隔的分类超平面
 - $C$较小：$\zeta_i$影响较小，因此限制条件可以被忽略，会得到较为宽间隔的分类超平面

5、结语
----

作为最直观简单的角度，最大化间隔的思想不仅带我们一步步走向SVM背后的原理，更让我们理解到SVM具有的良好泛化能力的原因。虽然我们现在得到的SVM只能处理线性的情况，但我觉得从最大间隔分类器的角度去看，走到这一步已经足够了。SVM还蕴含着很多有趣的性质和优点，我们会在其他角度的探寻中一一发现。

##Reference
1、本校AI课课件及参考资料：http://www.cs.rochester.edu/~jliu/CSC-242/svm.pdf， http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf
2、July博客：[支持向量机通俗导论](http://blog.csdn.net/v_july_v/article/details/7624837%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E9%80%9A%E4%BF%97%E5%AF%BC%E8%AE%BA)
3、PRML Chapter 7.