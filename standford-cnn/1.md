去年就想听听这“接地气”的课程了，但最终也没有坚持下来。今年既然有了在线的课程视频，我也开个笔记系列，好好地从基础开始学起吧。由于课程内容还是比较基础，这里只纪录一些个人觉得有趣或者有用的点，更多的内容大家直接去[网站](http://cs231n.stanford.edu/syllabus.html)上看吧，相信会有不少收获的。

----------


### Lecture 1：机器视觉历史简介
- 生物视觉的诞生可以追溯到距今五亿四千多万年前的物种大爆炸时期
- 公元16世纪出现了照相机的雏形纪录，人们开始考虑用仪器（光学镜片）来复制（duplicate）所看到的世界
- 1959年，Hubeli & Wiesel著名的工作尝试通过实验研究生物视觉系统的工作原理（测量视觉神经元对不同刺激的反应）。
  - 生物视觉系统中，底层视觉是由**简单的结构**（边缘等）组成的。
- 1963年，出现了机器视觉的先导性工作“Block world”。作者Larry Roberts在他的博士论文中首次提出通过计算机提取图片中物体的边缘。
- 1966年夏，MIT一个研究组提出的“The Sumer Vision Project”项目被认为是机器视觉诞生。（当时的教授认为：视觉是多么简单的东西，我们一个夏天就能够搞定！。。。）
- 1970年代，David Marr撰写了“Vision”一书。书中讲述了视觉系统具有一个**层次性的结构**（hierarchical）。
- 1987年，人们开始对图片中的真实物体进行识别，如David Lowe的工作。
- 1997年，Shi&Malik具有影响力的工作 Normalized Cut，开始研究称为”Perceptual Grouping“的问题：将图像分割成不同可被感知的小块或区域。
- 2001年，Viola&Jones发表了富有影响力的人脸检测工作。
  - 文中所用的特征是通过训练学习得到的
  - 第一次实现了实时的机器视觉算法（之前的算法都非常的慢）
  - 这个时期，机器视觉的关注点有所转移：从之前的视觉重构转到“识别”问题（更接近AI问题）
- 1999年，David Lowe提出了著名的**SIFT特征**，并用于目标识别当中。
- 2009年，Felzenswalb等人提出了**Deformable Part Model**，将目标分成各个相关的部分，并用SVM识别各个部分。（用于行人识别）
- 2006年左右，机器视觉领域逐渐成熟，学术界出现了一些公开的标准数据集，如：PASCAL，ImageNet。

### Lecture 2：图片分类基础
- 介绍了图片分类问题的挑战性（视角、光照、变形、遮挡、背景影响、类内差异等）
- 采用数据驱动（data-driven）的方法来解决，如：最邻近分类器（Nearest Neighbor classifier）
- 最邻近分类器的一个缺点：虽然训练速度很快（不需要训练），测试时的分类速度却随着训练数据量的增大而线性增大。然而在实际应用中我们一般认为测试速度比训练速度更为重要。（CNN则与之相反，训练时需要大量时间，但是测试时间却很短，与训练数据大小无关）
- 加速最邻近分类器的一些近似算法，如FLANN
- 通过验证集确定超参数（如k邻近算法中的k）
- 参数化模型（Parametric approach）：线性分类器 $f(x,W,b)=Wx+b$
- 线性分类器到底在做什么？
  - 利用训练得到的权值（weights）对特征的各个分量进行加权求和，得到该各类别的分数输出。如果我们直接拿图像像素值作为特征，则可以得到如下图的可视化结果：![1](http://img.blog.csdn.net/20160113144203339)
  - 如果我们将每张图像的特征看作高维空间中的一个点，线性分类器可以得到一些超平面，其中的权值即使这些超平面的法线方向，如下图：![2](http://img.blog.csdn.net/20160113144228287)


### Lecture 3：损失函数和优化问题
与上节课不同，这节课虽然也是介绍一些非常基本的概念，但其中对于损失函数的讨论比较有意思，值得一听。

- Multiclass SVM Loss：给定(数据，标签)对$(x_i,y_i)$ 和分类器输出$s=f(x_i,W)$，损失函数为 $L=\frac{1}{N}\sum_i L_i$，$L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+1)$
  - 权值正则化（Regularization）：$L(x,W)+\lambda R(W)$
- Softmax Loss：将分类器输出分数看作各类“未归一化”的指数概率（unnormalized log probabilities of the classes），$L_i=-log(\frac{e^{s_{y_i}}}{\sum_je^s_j})$
- SVM vs. Softmax
  - SVM 对于满足间隔要求的数据点更具有鲁棒性（当数据点远离间距时，微小的变动不会影响损失函数值（就是零））。SVM只对于数据空间中的一部分局部（margin附近）有影响，而Softmax对于整个空间都有影响。
  - 实际应用中两者差异不大
  - 一个可视化[demo](http://vision.stanford.edu/teaching/cs231n/linear-classify-demo)
- 优化：梯度下降
  - 数值梯度：近似值，计算慢，易实现 --> 常用于梯度检测
  - 微积分计算梯度：精确，快速，易犯错
- 一些不同优化算法的可视化[demo](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html)，非常好看。

